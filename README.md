# Detect AI-Generated text in a document

## ğŸ“Œ Overview

This project explores methods to **differentiate AI-generated text from human-written text** using a variety of machine learning models and engineered features. With the rise of Large Language Models (LLMs) such as GPT-4, Cohere, and LLaMA, distinguishing AI outputs from authentic human writing has become increasingly challenging.

The goal of this project is to evaluate and compare multiple approaches, including stylistic features, readability metrics, perplexity, and compression-based features to build reliable models for AI text detection.

---

## ğŸ¯ Objectives

* Build a pipeline to preprocess and clean text datasets.
* Engineer linguistic and structural features (e.g., **perplexity, Gunning Fog Index, SMOG Index, compression factors**).
* Train and evaluate machine learning models (**SVM, XGBoost, MLP**) for classification.
* Compare performance across internal and external datasets to assess generalization.
* Provide insights into which features and methods are most effective for AI text detection.

---

## ğŸ—‚ï¸ Datasets

* **Kaggle Competition Dataset** â€“ 1,000 rows of essays.
* **DAIGT Train Dataset** â€“ 5,000 rows of high-quality essays.
* **Generated Essays** â€“ 500 samples generated using GPT-4, Cohere, etc.
* **External Test Dataset (Hugging Face)** â€“ 15,000 rows for robust generalization.

---

## ğŸ› ï¸ Features Extracted

* **Vocabulary Richness** â€“ Lexical diversity of the text.
* **Readability Metrics** â€“ Gunning Fog Index, SMOG Index.
* **Polarity & Subjectivity** â€“ Sentiment orientation and subjectivity levels.
* **Perplexity** â€“ Computed using GPT-2 to measure predictability of text.
* **Compression Factors** â€“ Using **Lempel-Ziv** and **Gzip** algorithms to capture redundancy.

---

## ğŸ¤– Models Implemented

1. **Support Vector Machines (SVM)**

   * Explored kernels (linear, RBF) and hyperparameters with Grid Search & Optuna.

2. **XGBoost**

   * Gradient boosting with DART booster, early stopping, and class weight adjustments.

3. **Multi-Layer Perceptron (MLP)**

   * Neural network with dropout, L2 regularization, and tuned with Hyperband.

---

## ğŸ“Š Results

| Model       | Accuracy | ROC-AUC   | Notes                                                         |
| ----------- | -------- | --------- | ------------------------------------------------------------- |
| **XGBoost** | \~89%    | 0.95      | Strong performance but some overfitting observed.             |
| **SVM**     | \~87%    | 0.86â€“0.89 | Consistent generalization across datasets.                    |
| **MLP**     | \~85â€“92% | 0.93+     | Excellent generalization, especially on Hugging Face dataset. |

ğŸ”¹ Key takeaway: **Perplexity, SMOG Index, and Gunning Fog Index** were the most impactful features for classification.

---

## ğŸš€ Future Work

* Incorporate **transformer-based embeddings (BERT, RoBERTa)** for semantic features.
* Train on datasets generated by **newer LLMs (e.g., GPT-5)**.
* Explore **adversarial robustness testing** for stronger generalization.
* Deploy as a real-time **AI text detection tool** for academic and professional use.

---

## ğŸ“œ References

* Kaggle: [LLM - Detect AI Generated Text](https://www.kaggle.com)
* Hugging Face: [Human vs AI Generated Text Dataset](https://huggingface.co/datasets)
* Wu et al. (2023): *Survey on LLM-Generated Text Detection*
* Antoun et al. (2023): *Towards Robust Detection of Language Model Generated Text*

---

Would you like me to also generate a **shorter version** of this README (for GitHub front page) thatâ€™s more concise and user-facing, while keeping this detailed one as `REPORT.md`?
